{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis of Menu Items in Yelp Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division # Use Python 3-style division\n",
    "import ujson\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import hashlib\n",
    "from pprint import pprint\n",
    "\n",
    "from num2words import num2words\n",
    "import string\n",
    "import nltk.corpus\n",
    "from unidecode import unidecode\n",
    "from nltk import word_tokenize\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "import codecs\n",
    "import json\n",
    "import nltk\n",
    "import editdistance\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "from ConfigParser import SafeConfigParser\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vader_analyzer = SentimentIntensityAnalyzer(\"/usr/local/share/nltk_data/sentiment/vader_lexicon/vader_lexicon.txt\")\n",
    "\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for loading Pittsburgh Yelp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadRestaurantsData():\n",
    "    # Load restaurants data\n",
    "    restaurants = {}\n",
    "    with open(\"pittsburgh_restaurants.json\", 'r') as f:\n",
    "        for line in f:\n",
    "            res = ujson.loads(line)\n",
    "            restaurants[res[\"business_id\"]] = res\n",
    "\n",
    "    return restaurants\n",
    "\n",
    "\n",
    "def loadReviewsData():\n",
    "    # Load Review data\n",
    "    maxReview = sys.maxint\n",
    "    n = 0\n",
    "    reviews = {}\n",
    "    reviewers = {}\n",
    "\n",
    "    with open(\"pittsburghReviews.json\", 'r') as f:\n",
    "        for line in f:\n",
    "            review = ujson.loads(line)\n",
    "            reviews[review['review_id']] = review\n",
    "\n",
    "            restaurant = restaurants.get(review[\"business_id\"], None)\n",
    "            if not restaurant:\n",
    "                continue\n",
    "\n",
    "            # Collect stats for reviewers\n",
    "            reviewSummary = reviewers.get(review[\"user_id\"],\n",
    "                                          {\"reviewCount\":0,\n",
    "                                           \"stars\":Counter(), \"wkdays\":Counter(),\n",
    "                                           \"neighbors\":Counter(), \"postalCodes\":Counter(),\n",
    "                                           \"useful\":0, \"categories\":Counter(),\n",
    "                                           \"businessIds\":Counter()})\n",
    "\n",
    "            reviewSummary[\"reviewCount\"] += 1\n",
    "            reviewSummary[\"stars\"][int(review[\"stars\"])] += 1\n",
    "            reviewSummary[\"wkdays\"][datetime.date(datetime.strptime(review[\"date\"], \"%Y-%m-%d\")).isoweekday()] += 1\n",
    "            reviewSummary[\"useful\"] += int(review[\"useful\"])\n",
    "            reviewSummary[\"businessIds\"][review[\"business_id\"]] += 1\n",
    "\n",
    "            reviewSummary[\"postalCodes\"][restaurant[\"postal_code\"]] += 1\n",
    "\n",
    "            neighbor = restaurant[\"neighborhood\"]\n",
    "            if not neighbor:\n",
    "                neighbor = \"None\"\n",
    "            reviewSummary[\"neighbors\"][neighbor] += 1\n",
    "\n",
    "            for category in restaurant[\"categories\"]:\n",
    "                if category not in [\"Restaurants\", \"Food\"] :\n",
    "                    reviewSummary[\"categories\"][category] += 1\n",
    "\n",
    "            reviewers[review[\"user_id\"]] = reviewSummary\n",
    "\n",
    "            n += 1\n",
    "            if n == maxReview:\n",
    "                break\n",
    "\n",
    "    return reviews, reviewers\n",
    "\n",
    "def buildRestaurantReviewIds(reviews):\n",
    "    # Build an inverted index so that we can list all the review ids of a restaurant quickly\n",
    "    restaurantReviewIds = {}\n",
    "    for reviewId in reviews.keys():\n",
    "        review = reviews[reviewId]\n",
    "        busId = review['business_id']\n",
    "        reviewIds = restaurantReviewIds.get(busId, set())\n",
    "        reviewIds.add(reviewId)\n",
    "        restaurantReviewIds[busId] = reviewIds\n",
    "\n",
    "    return restaurantReviewIds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "restaurants = loadRestaurantsData()\n",
    "reviews, reviewers = loadReviewsData()\n",
    "restaurantReviewIds = buildRestaurantReviewIds(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lemmatize - return the dictionary form of a word: e.g. dogs -> dog\n",
    "def get_lemma(word):\n",
    "    try:\n",
    "        return wnl.lemmatize(word)\n",
    "    except UnicodeEncodeError:\n",
    "        return word\n",
    "\n",
    "\n",
    "# Return all possible n-grams from a list of words\n",
    "def find_ngrams(input_list, n):\n",
    "    n = min(len(input_list), n)\n",
    "    return zip(*[input_list[i:] for i in range(n)])\n",
    "\n",
    "    \n",
    "# Stopwords\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Add some commonly used non-English stopwords\n",
    "stopwords.add(\"en\")\n",
    "stopwords.add(\"et\")\n",
    "\n",
    "\n",
    "# Do a fuzzy comparison between word and target\n",
    "def fuzzyMatch(word, target):\n",
    "    if word == target:\n",
    "        return True\n",
    "\n",
    "    if word[0] != target[0]:\n",
    "        return False\n",
    "\n",
    "    n = len(word)\n",
    "\n",
    "    # Short words require full match\n",
    "    # Avoid matching Sunday with Sundae\n",
    "    # Avoid matching plate with pate\n",
    "    if n <= 4 or word == \"sunday\" or word == \"plate\":\n",
    "        return word == target\n",
    "\n",
    "    dist = editdistance.eval(word, target)\n",
    "    if n <= 6:\n",
    "        return dist <= 1\n",
    "    elif n <= 12:\n",
    "        return dist <= 2\n",
    "    else:\n",
    "        return dist <= 3\n",
    "\n",
    "\n",
    "# Return whether \"word\" fuzzy-matches one of the targets\n",
    "def fuzzyMatchOneTarget(word, targets):\n",
    "    for target in targets:\n",
    "        match = fuzzyMatch(word, target)\n",
    "        if match:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "isDebug = False\n",
    "def debugPrint(s):\n",
    "    if isDebug:\n",
    "        print s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Help functions for analyzing review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A quicker way to do POS tagging\n",
    "tagger = PerceptronTagger()\n",
    "\n",
    "# Unwanted word types\n",
    "unwantedWordTypes = []\n",
    "\n",
    "# See http://www.comp.leeds.ac.uk/amalgam/tagsets/upenn.html for the definition of these tags\n",
    "unwantedWordTypes.append(\"POS\")\n",
    "unwantedWordTypes.append(\"EX\")\n",
    "unwantedWordTypes.append(\"DT\")\n",
    "unwantedWordTypes.append(\"to\")\n",
    "unwantedWordTypes.append(\"WDT\")\n",
    "unwantedWordTypes.append(\"WP\")\n",
    "unwantedWordTypes.append(\"WP$\")\n",
    "unwantedWordTypes.append(\"WRB\")\n",
    "\n",
    "\n",
    "# Normalize certain dish names\n",
    "normalizedDishNames = {\n",
    "    \"hamburger\": \"burger\",\n",
    "    \"bier\": \"beer\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess some words\n",
    "def preprocessWords(s, restaurantName = None):\n",
    "    if not s or len(s) == 0:\n",
    "        return []\n",
    "    \n",
    "    s = s.lower()  # Lowercase it\n",
    "\n",
    "    if restaurantName:\n",
    "        # Remove restaurant name in a string, otherwise we will always get a match whenever someone\n",
    "        # mentioned the restaurant name in the review\n",
    "        if isinstance(restaurantName, list):\n",
    "            for name in restaurantName:\n",
    "                if name:\n",
    "                    s = s.replace(name, \"\")\n",
    "        else:\n",
    "            s = s.replace(restaurantName, \"\")\n",
    "\n",
    "    # Convert certain common word variations\n",
    "    s = s.replace(\"-a-\", \" a \")\n",
    "    s = s.replace('\"', '')\n",
    "    s = s.replace(\"'s \", ' ')\n",
    "    s = s.replace(\"-o-\", \" of \")\n",
    "    s = s.replace(\"/\", \" / \")\n",
    "    s = s.replace(\"&\", \" and \")\n",
    "    s = s.replace(\"'n \", \" and \")\n",
    "    s = s.replace(\" n'\", \" and \")\n",
    "    s = s.replace(\"'n'\", \" and \")\n",
    "    s = s.replace(\"-n-\", \" and \")\n",
    "    s = s.replace(\"'\", \" \")\n",
    "    s = s.replace(\"*\", \" \")\n",
    "    s = s.replace(\"-\", \" - \")\n",
    "\n",
    "    s = unidecode(s)  # Convert accented chars to plain English chars\n",
    "    \n",
    "    tags = tagger.tag(word_tokenize(s))  # Get POS tag of the words\n",
    "\n",
    "    tags = [t for t in tags if t[1] not in unwantedWordTypes] # Remove type of word we don't want\n",
    "\n",
    "    tags = [t for t in tags if t[0] not in stopwords] # Remove stopwords\n",
    "\n",
    "    tags = [t for t in tags if t[0] not in string.punctuation] # Remove punctuations\n",
    "\n",
    "    s = [t[0] for t in tags]\n",
    "\n",
    "    # Convert numbers to words\n",
    "    s = [w if not w.isdigit() else num2words(int(w))  for w in s]\n",
    "\n",
    "    s = [get_lemma(w) for w in s]  # Convert each word to its Dictionary version\n",
    "\n",
    "    s = [normalizedDishNames.get(w, w) for w in s] # Normalize certain dish name\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Before we decide whether a review text n-gram matches a menu item, we first check the menu section which\n",
    "# contains the menu item.  If the section contains one of the keywords found in the sectionKeywords below, then\n",
    "# n-gram has to contain that keyword as well.\n",
    "#\n",
    "# E.g. let's say a menu contains a section called \"Sandwich\" and a section called \"Pie\", and under each section\n",
    "# there is a menu item called \"Beef\".\n",
    "#\n",
    "# In the review, an n-gram \"ate a beef sandwich\" will match both menu item names.  We will use the section of each\n",
    "# menu item to determine whether we match a beef sandwich or a beef pie.\n",
    "sectionKeywords = [\n",
    "    \"bbq\", \"grill\", \"stew\", \"skillet\", \"casserole\",\n",
    "\n",
    "    \"pasta\", \"pizza\", \"flatbread\", \"rice\", \"bean\", \"noodle\",\n",
    "\n",
    "    \"salad\",\n",
    "\n",
    "    \"soap\",\n",
    "\n",
    "    \"casserole\", \"burger\", \"sandwich\", \"pie\", \"tart\", \"wrap\",\n",
    "\n",
    "    \"lamb\", \"beef\", \"pork\", \"chicken\", \"schnitzel\",\n",
    "    \"mussel\", \"clam\", \"oyster\",\n",
    "\n",
    "    # Drinks\n",
    "    \"beer\", \"coffee\",\n",
    "\n",
    "    # Mexican\n",
    "    \"nacho\", \"Quesadilla\", \"taco\", \"Enchilada\", \"Burrito\", \"fajita\", \"Chimichanga\",\n",
    "\n",
    "    # Japanese\n",
    "    \"sushi\", \"nigiri\", \"sashimi\"\n",
    "]\n",
    "\n",
    "# Make sure all words are lower case and lemmatized\n",
    "sectionKeywords = [get_lemma(w.lower()) for w in sectionKeywords]\n",
    "\n",
    "# These are some generic section names which we don't expect to be found in the actual review text when\n",
    "# the reviewer is mentioning a menu item.\n",
    "genericSectionNames = [\"specialty\", \"dessert\", \"appetizer\", \"favorite\", \"drink\", \"entree\", \"main\", \"course\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process a json file (fn) which contains the sections and menu items of a restaurant, and\n",
    "# return a list of data structures used in review analysis.\n",
    "def parseMenuItems(fn, restaurants):\n",
    "    menuItems = []\n",
    "    menuSections = []\n",
    "\n",
    "    with codecs.open(fn, \"r\", \"utf-8\") as f:\n",
    "        menuInfo = json.load(f)\n",
    "\n",
    "        resId = menuInfo[\"id\"]\n",
    "        restaurantName = menuInfo[\"name\"]\n",
    "\n",
    "        # Does this restaurant have an alt. name?\n",
    "        restaurantInfo = restaurants.get(resId, None)\n",
    "        altName = None\n",
    "        if restaurantInfo:\n",
    "            name = restaurantInfo[\"name\"]\n",
    "            if name != restaurantName:\n",
    "                altName = name.lower()\n",
    "\n",
    "        restaurantNames = [unidecode(restaurantName.lower()), unidecode(altName) if altName else altName]\n",
    "\n",
    "        for section in menuInfo[\"sections\"]:\n",
    "            sectionName = section[\"name\"]\n",
    "            preprocessedName = preprocessWords(sectionName, restaurantNames)\n",
    "\n",
    "            # Check if the section name contains nothing but generic terms\n",
    "            onlyGenericTerms = False\n",
    "            n = 0\n",
    "            for word in preprocessedName:\n",
    "                if word in genericSectionNames:\n",
    "                    n += 1\n",
    "\n",
    "            if n == len(preprocessedName):\n",
    "                onlyGenericTerms = True\n",
    "\n",
    "            # Does the section name contain one of the main dishes?\n",
    "            matchedSectionKeyword = None\n",
    "            for sectionKeyword in sectionKeywords:\n",
    "                if sectionKeyword in preprocessedName:\n",
    "                    matchedSectionKeyword = sectionKeyword\n",
    "                    break\n",
    "\n",
    "            lastSection = {\n",
    "                \"name\": sectionName,\n",
    "                \"preprocessedName\": preprocessedName,\n",
    "                \"sectionKeyword\": matchedSectionKeyword,\n",
    "                \"onlyGenericTerms\": onlyGenericTerms\n",
    "            }\n",
    "\n",
    "            menuSections.append(lastSection)\n",
    "\n",
    "            for item in section[\"items\"]:\n",
    "                name = item[\"name\"]\n",
    "                desc = item.get(\"desc\", \"\")\n",
    "                menuItems.append({\"name\": name,\n",
    "                                  \"desc\": desc,\n",
    "                                  \"preprocessedName\": set(preprocessWords(name, restaurantNames)),\n",
    "                                  \"preprocessedDesc\": set(preprocessWords(desc, restaurantNames)),\n",
    "                                  \"section\": lastSection\n",
    "                                 })\n",
    "\n",
    "    return restaurantName, altName, resId, menuItems, menuSections\n",
    "\n",
    "\n",
    "# Try to match menu items in a 'preprocessed sentence'\n",
    "def match_menu_items_in_sent(menuItems, menuSections, preprocessedSent, ngramLen):\n",
    "    matched_items = {}\n",
    "    section_matched_items = set()\n",
    "\n",
    "    # Loop thru all n-grams in the sentence\n",
    "    for ngram in find_ngrams(preprocessedSent, ngramLen):\n",
    "        ngram = set(ngram)  # Remove duplicate terms\n",
    "        item_match = False\n",
    "\n",
    "        # See if it matches a menu item\n",
    "        for item in menuItems:\n",
    "            itemWords = item[\"preprocessedName\"]\n",
    "            sectionWords = item[\"section\"][\"preprocessedName\"]\n",
    "\n",
    "            match = False\n",
    "            sectionKeywordMatch = False\n",
    "\n",
    "            # See if our n-gram matches certain number of terms in the menu item name\n",
    "            # If menu item has:\n",
    "            # - <= 2 terms, the ngram needs to match all the terms\n",
    "            # - 3 or 4 terms, the ngram needs to match at least two terms\n",
    "            # - >= 5 terms, the ngram needs to match at least three terms\n",
    "            n_matched = sum([1 if fuzzyMatchOneTarget(w, itemWords) else 0 for w in ngram])\n",
    "            #debugPrint(\"ngram:{}, itemWords:{}, n_matched:{}\".format(ngram, itemWords, n_matched))\n",
    "\n",
    "            itemLen = len(itemWords)\n",
    "            matchItemName = False\n",
    "\n",
    "            if itemLen <= 2:\n",
    "                if n_matched >= itemLen:\n",
    "                    matchItemName = True\n",
    "            elif itemLen <= 4:\n",
    "                if n_matched >= 2:\n",
    "                    matchItemName = True\n",
    "            else:\n",
    "                if n_matched >= 3:\n",
    "                    matchItemName = True\n",
    "\n",
    "            if matchItemName:\n",
    "                if itemLen >= 3 and n_matched == itemLen:\n",
    "                    # For item with long name and we have a full match, consider it a good match\n",
    "                    match = True\n",
    "                else:\n",
    "                    # For dishes which belong to certain type of dish (e.g. sandwich), we need to check if the n-gram\n",
    "                    # contains that dish name\n",
    "                    sectionKeyword = item[\"section\"][\"sectionKeyword\"]\n",
    "\n",
    "                    if sectionKeyword:\n",
    "                        # See if the ngram matches at least a word in the sectionKeyword we identified in the section\n",
    "                        # to which the dish belongs\n",
    "                        if sectionKeyword in ngram:\n",
    "                            debugPrint(\"! sectionKeyword matched !\")\n",
    "                            sectionKeywordMatch = True\n",
    "                            match = True\n",
    "                    else:\n",
    "                        # We don't have a sectionKeyword related to the section.\n",
    "\n",
    "                        # If this case, first see if the ngram matches at least a word in section name\n",
    "\n",
    "                        if item[\"section\"][\"onlyGenericTerms\"]:\n",
    "                            # Its section contains only generic terms (e.g. \"desserts).\n",
    "                            # In this case we don't match section and we assume we have a match\n",
    "                            debugPrint(\"! No need for section match !\")\n",
    "                            match = True\n",
    "                        else:\n",
    "                            n_match_section = sum([1 if w in sectionWords else 0 for w in ngram])\n",
    "                            if n_match_section >= 1:\n",
    "                                debugPrint(\"! section match !\")\n",
    "                                match = True\n",
    "\n",
    "                #if not match:\n",
    "                    # Last try: see if the n-gram matches at least two terms in the item description\n",
    "                #    if len(ngram & item[\"preprocessedDesc\"]) >= 2:\n",
    "                #        debugPrint(\"! desc match !\")\n",
    "                #        match = True\n",
    "\n",
    "            elif n_matched > 0:\n",
    "                # if we match at least one word, but we couldn't match half of the words in the item name,\n",
    "                # see if all the words in the ngram are found in the description\n",
    "                if item[\"preprocessedDesc\"] and len(ngram & set(item[\"preprocessedDesc\"])) == len(ngram):\n",
    "                    debugPrint(\"! desc full match only !\")\n",
    "                    match = True\n",
    "\n",
    "            if match:\n",
    "                debugPrint(u\"[DEBUG: n_matched={} ngram={}; Menu: {}-{}; {} - {}]\".format(n_matched, ngram,\n",
    "                                                          sectionWords, itemWords,\n",
    "                                                           item[\"section\"], item[\"name\"]))\n",
    "\n",
    "                itemName = u\"{} ({})\".format(item[\"name\"], item[\"section\"][\"name\"])\n",
    "                score = n_matched + 2*sectionKeywordMatch\n",
    "\n",
    "                if itemName not in matched_items or score > matched_items[itemName][\"score\"]:\n",
    "                    matched_items[itemName] = {\"score\": score, \"item\": item}\n",
    "\n",
    "                # Add the item's section to the matched-section set\n",
    "                section_matched_items.add(item[\"section\"][\"name\"])\n",
    "\n",
    "                item_match = True\n",
    "\n",
    "        if not item_match:\n",
    "            # There is no item match for this ngram.  See if it matches a section\n",
    "            for section in menuSections:\n",
    "                sectionWords = section[\"preprocessedName\"]\n",
    "                if not section[\"onlyGenericTerms\"] and sectionWords:\n",
    "                    n_matched = sum([1 if fuzzyMatchOneTarget(w, sectionWords) else 0 for w in ngram])\n",
    "                    if n_matched > 0:\n",
    "                        section_matched_items.add(section[\"name\"])\n",
    "\n",
    "    return matched_items, section_matched_items\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try to match menu items in a review\n",
    "def match_menu_items_in_review(matches, matchedSents, review, menuItems, menuSections, ngramLen):\n",
    "    total_matches = 0\n",
    "\n",
    "    reviewText = review[\"text\"]\n",
    "    reviewId = review[\"review_id\"]\n",
    "    reviewDate = datetime.strptime(review[\"date\"], \"%Y-%m-%d\")\n",
    "\n",
    "    sents = nltk.sent_tokenize(reviewText) # break our review into sentences\n",
    "    preprocessedSents = [preprocessWords(s) for s in sents] # preprocess each sentence\n",
    "\n",
    "    for sent, processedSent in zip(sents, preprocessedSents):\n",
    "        matched_items, section_matched_items = match_menu_items_in_sent(menuItems, menuSections, \n",
    "                                                                        preprocessedSent, ngramLen)\n",
    "\n",
    "        hasMatch = False\n",
    "        if len(matched_items) > 0 or len(section_matched_items) > 0:\n",
    "            hasMatch = True\n",
    "\n",
    "        if hasMatch:\n",
    "            # TODO: need to handle the case where the item name contains a strong sentiment word\n",
    "            # e.g. \"angry tiki dog\"\n",
    "\n",
    "            # TODO: sometimes we need to consider the next sentence(s) as well.\n",
    "\n",
    "            sentiment = vader_analyzer.polarity_scores(sent)\n",
    "            print\n",
    "            print(u\"{}\".format(sent))\n",
    "            print\n",
    "            print \"---- Sentiment:\"\n",
    "            print sentiment\n",
    "\n",
    "        else:\n",
    "            print(u\"{}\".format(sent))\n",
    "\n",
    "        # TODO: if an n-gram matches multiple items, we should pick only one\n",
    "\n",
    "        if len(matched_items) > 0:\n",
    "            # Save up the matched sentence\n",
    "            if reviewId not in matchedSents:\n",
    "                matchedSents[reviewId] = set()\n",
    "\n",
    "            matchedSents[reviewId].add(sent)\n",
    "            sentMd5 = hashlib.md5(sent.encode('utf-8')).hexdigest()\n",
    "\n",
    "            total_matches += len(matched_items)\n",
    "            print \"---- Matched Items:\"\n",
    "            for key, item in matched_items.iteritems():\n",
    "                print key\n",
    "                info = matches.get(key, {\"shortName\": item[\"item\"][\"name\"],\n",
    "                                         \"scores\": [], \"dates\": [], \"reviewIds\": [], \"matchedSentMd5\": []})\n",
    "                info.get(\"scores\").append(sentiment['compound'])\n",
    "                info.get(\"dates\").append(reviewDate)\n",
    "                info.get(\"reviewIds\").append(reviewId)\n",
    "                info.get(\"matchedSentMd5\").append('sent-' + sentMd5)\n",
    "                matches[key] = info\n",
    "\n",
    "        if len(section_matched_items) > 0:\n",
    "            print \"---- Matched Sections:\"\n",
    "            for item in sorted(list(section_matched_items)):\n",
    "                print item\n",
    "\n",
    "        if hasMatch:\n",
    "            print \"----\\n\"\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Menu Item Matching Process\n",
    "\n",
    "In order to analyze the sentiment of a menu item in a review text, first we have to identify which menu item(s) are mentioned in a review body.  Below gives a high level summary of this process.  \n",
    "\n",
    "---\n",
    "### Step 1: Break down the review body into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Great beer, great atmosphere, great food, & a good portion size at a decent price. The evening started out with a glass of their smooth tasting Gulden Draak Ale. The Mediterranean nachos tasted awesome, while the shrimp and langostino pizza was loaded w plenty of shrimp, lobster and flavor.'\n"
     ]
    }
   ],
   "source": [
    "reviewText = \\\n",
    "\"Great beer, great atmosphere, great food, & a good portion size at a decent price. The evening started \\\n",
    "out with a glass of their smooth tasting Gulden Draak Ale. The Mediterranean nachos tasted awesome, while the shrimp \\\n",
    "and langostino pizza was loaded w plenty of shrimp, lobster and flavor.\"\n",
    "\n",
    "pprint(reviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Great beer, great atmosphere, great food, & a good portion size at a decent price.',\n",
      " 'The evening started out with a glass of their smooth tasting Gulden Draak Ale.',\n",
      " 'The Mediterranean nachos tasted awesome, while the shrimp and langostino pizza was loaded w plenty of shrimp, lobster and flavor.']\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(reviewText)\n",
    "pprint(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: Preprocess each sentence\n",
    "In an ideal world, when a reviewer mentions a dish, it will have a perfect match with what is printed in the restaurant menu.  However, in reality there are many ways to break this assumption, with *some* listed below:\n",
    "\n",
    "\n",
    "| Menu Item        | What the reviewer wrote instead  | Problem |\n",
    "| :------------- |:-------------| :-------------| \n",
    "| Nachos      | Nacho      | Singular vs Plural |\n",
    "| Pâté | Pate      | Accented characters |\n",
    "| Spicy Hamburger | Spicy Burger | A similar word is used instead |\n",
    "| Mac-n-Cheese | Mac and Cheese | Short forms |\n",
    "| Lindeman's Framboise Cheesecake | Lindeman Framboise Cheesecake | The apostrophe+s is missing |\n",
    "\n",
    "\n",
    "\n",
    "In order to make it easier to match a menu iten mentioned in a review, we introduce a *preprocessing* step, which will:\n",
    "- Lowercase the sentence\n",
    "- Remove stopwords and punctuations\n",
    "- Convert numbers to words (e.g. 20 -> twenty)\n",
    "- Convert each word to its dictionary form (i.e. lemmatize) (e.g. nachos -> nacho)\n",
    "- Convert accented characters to English characters (e.g. Pâté -> Pate)\n",
    "- Normalize certain dish names (e.g. hamburger -> burger)\n",
    "- Normalize the short-form of certain words (e.g. Mac & Cheese -> Mac and Cheese)\n",
    "- Identify the Part-of-Speech (POS) tag of each word and remove it if it belongs to certain types of tag (e.g. all determiner (i.e. the, all, both, each and every, etc.) are removed)\n",
    "\n",
    "For example, after preprocessing, the sentence *\"The Mediterranean nachos tasted awesome, while the mac-n-cheese was just so so.\"* becomes `['mediterranean', 'nacho', 'tasted', 'awesome', 'mac', 'cheese']`.  \n",
    "\n",
    "Please note that we will *preprocess* all **reviews** as well as **restaurant menu items**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Additional problems faced during matching\n",
    "Before we cover the next step, we need to first look at some *additional* problems faced during matching:\n",
    "\n",
    "| Menu Item        | What the reviewer wrote instead  | Problem |\n",
    "| :------------- |:-------------| :-------------| \n",
    "| Mediterranean Nachos      | Mediteranean Nachos | Spelling mistake |\n",
    "| Bacon Ranch Chicken Sandwich | Ranch Chicken Bacon Sandwich      | The order of words are different |\n",
    "| Black Forest Guinness Brownie      | Black Forest Brownie      | Some word(s) are missing |\n",
    "\n",
    "\n",
    "How to tackle these problems:\n",
    "- Spelling mistake\n",
    "    - When comparing two words, we measure the edit-distance between them and allow for certain degree of difference.\n",
    "- The order of words are different\n",
    "    - When looking for an item match in a sentence, we iterate all the 4-grams of the sentence, and see how many words from a menu item are included in the 4-gram *regardless* of ordering.\n",
    "- Some word(s) are missing\n",
    "    - In the comparison using 4-gram, *sometimes* we call it a match when only some of the words in the menu item is found.  E.g. the 4-gram `['order', 'Black', 'Forest', 'Brownie']` will match the menu item *Black Forest Guinness Brownie* even though only 3 words are found.\n",
    "\n",
    "With these problems explained, let us introduction the next step.\n",
    "\n",
    "---\n",
    "### Step 4: Try to match menu item(s) in each sentence:\n",
    "- Iterate through all the 4-gram in each *preprocessed* sentence, and use the aforementioned ways to compare it with each *preprocessed* item from that restaurant.\n",
    "- Sometimes we will also make use of the description a menu item to check if a 4-gram is a match.\n",
    "\n",
    "Moreover, on a menu multiple items could share the same name.  For example, on one menu the name *Shrimp and Avacado* is listed under both the *Sandwich section* and the *Salad section*.  In order to decide which item the review is talking about, under *some* conditions we require a 4-gram to contain the *section words* as well before we can call it a match.\n",
    "\n",
    "---\n",
    "Lastly, the above gives an overview of the process of menu item matching.  Please bear in mind that all these steps and tricks cannot *guarantee* a perfect match, but they will help to improve the accuracy of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
